<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="shortcut icon" href="/sites/default/files/favicon.png" type="image/x-icon" />
		<title>2012 IEEE VAST Papers  | IEEE VIS</title>
    <!--<title> | IEEE VIS </title>-->
    <link type="text/css" rel="stylesheet" media="all" href="/modules/node/node.css" />
<link type="text/css" rel="stylesheet" media="all" href="/modules/system/defaults.css" />
<link type="text/css" rel="stylesheet" media="all" href="/modules/system/system.css" />
<link type="text/css" rel="stylesheet" media="all" href="/modules/system/system-menus.css" />
<link type="text/css" rel="stylesheet" media="all" href="/modules/user/user.css" />
<link type="text/css" rel="stylesheet" media="all" href="/sites/ieeevis.org/modules/cck/theme/content-module.css" />
<link type="text/css" rel="stylesheet" media="all" href="/sites/ieeevis.org/modules/ctools/css/ctools.css" />
<link type="text/css" rel="stylesheet" media="all" href="/sites/ieeevis.org/modules/date/date.css" />
<link type="text/css" rel="stylesheet" media="all" href="/sites/ieeevis.org/modules/date/date_popup/themes/datepicker.css" />
<link type="text/css" rel="stylesheet" media="all" href="/sites/ieeevis.org/modules/date/date_popup/themes/timeentry.css" />
<link type="text/css" rel="stylesheet" media="all" href="/sites/ieeevis.org/modules/filefield/filefield.css" />
<link type="text/css" rel="stylesheet" media="all" href="/misc/farbtastic/farbtastic.css" />
<link type="text/css" rel="stylesheet" media="all" href="/sites/ieeevis.org/modules/cck/modules/fieldgroup/fieldgroup.css" />
<link type="text/css" rel="stylesheet" media="all" href="/sites/ieeevis.org/themes/visweek/style.css" />
<link type="text/css" rel="stylesheet" media="all" href="/sites/ieeevis.org/themes/visweek/menutree.css" />
<link type="text/css" rel="stylesheet" media="all" href="/sites/ieeevis.org/themes/visweek/visweek.year.css" />
<link type="text/css" rel="stylesheet" media="all" href="/sites/ieeevis.org/themes/visweek/visweek.week.css" />
<link type="text/css" rel="stylesheet" media="all" href="/sites/ieeevis.org/themes/visweek/visweek.day.css" />
<link type="text/css" rel="stylesheet" media="all" href="/sites/ieeevis.org/themes/visweek/visweek.popup.css" />
<link type="text/css" rel="stylesheet" media="all" href="/sites/ieeevis.org/themes/visweek/visweek.login.css" />
<link type="text/css" rel="stylesheet" media="all" href="/sites/ieeevis.org/themes/visweek/visweek.twig.css" />
<link type="text/css" rel="stylesheet" media="all" href="/sites/ieeevis.org/themes/visweek/visweek.gowri.css" />
<link type="text/css" rel="stylesheet" media="all" href="/sites/ieeevis.org/themes/visweek/visweek.session.css" />
<link type="text/css" rel="stylesheet" media="print" href="/sites/ieeevis.org/themes/visweek/print.css" />
    <script type="text/javascript" src="/misc/jquery.js"></script>
<script type="text/javascript" src="/misc/drupal.js"></script>
<script type="text/javascript" src="/sites/ieeevis.org/modules/views_accordion/views-accordion.js"></script>
<script type="text/javascript" src="/sites/ieeevis.org/themes/visweek/js/slide.js"></script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
jQuery.extend(Drupal.settings, { "basePath": "/", "views_accordion": { "views-accordion-paper_session-page_1": { "keeponeopen": 0, "speed": 500, "firstopen": 0, "grouping": 1, "togglelinks": 1, "autocycle": 0, "autocyclespeed": 5000, "display": "div.view-display-id-page_1", "usegroupheader": 0, "header": "views-field-title" } } });
//--><!]]>
</script>
<script type="text/javascript">
<!--//--><![CDATA[//><!--
if (Drupal.jsEnabled) { $(document).ready(function() { $('body').addClass('yui-skin-sam'); } ); };
//--><!]]>
</script>
            <script type="text/javascript" src="http://ieeevis.org/sites/all/themes/year/js/jquery_cookie_plugin.js"></script>
    <script type="text/javascript" src="http://ieeevis.org/sites/all/themes/year/js/leftsidebar.js"></script>
    <script> jQuery(function(){
        jQuery('#left-nav').collapsibleNav();
        });
     </script>
    <!--[if lt IE 7]>
      <link type="text/css" rel="stylesheet" media="all" href="/sites/ieeevis.org/themes/visweek/fix-ie.css" />     
    <![endif]-->
  </head>
  <body onload="init()" class="sidebars" >
<!--[if IE]><div id="IEroot"><![endif]-->
<!-- Layout -->
          <div id="header-region" class="clear-block">
       <div id="header_bar" class="clear-block"><div id="block-block-3" class="clear-block block block-block">
<!--
-->
  <div class="content"><div id="header_text">14 - 19 OCTOBER, 2012. SEATTLE, WASHINGTON, USA</div><img src="http://ieeevis.org/sites/visweek.vgtc.org/files/header/visweek12-header.jpg"></div>
</div>
</div>
  </div>
    <div id="wrapper">
    <div id="container" class="clear-block">
      <div id="header">
        <div id="logo-floater">
                </div>

                                                    
      </div> <!-- /header -->
              <div id="sidebar-left" class="sidebar">
<div style="width:40%;padding:5%;float:left">
<iframe src="//www.facebook.com/plugins/like.php?href=https%3A%2F%2Fwww.facebook.com%2Fieeevis&amp;send=false&amp;layout=button_count&amp;width=450&amp;show_faces=false&amp;action=like&amp;colorscheme=light&amp;font&amp;height=21" scrolling="no" frameborder="0" style="border:none; overflow:hidden; width:450px; height:21px;" allowTransparency="true"></iframe>
</div>
<div style="width:40%;padding:5%;margin-left:50%;text-align:right">
<a href="https://twitter.com/ieeevis" class="twitter-follow-button" data-show-count="false" data-show-screen-name="false">Follow @ieeevis</a>
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
</div>
                    <div id="block-nodeasblock-1155" class="clear-block block block-nodeasblock">
<!--
  <h2><a href="/year/2012/left-sidebar-2012">Left Sidebar 2012</a></h2>
-->
  <div class="content"><div class="field field-type-number-integer field-field-conf-year">
    <div class="field-items">
            <div class="field-item odd">
                    2012        </div>
        </div>
</div>
<div id="left-nav"><div class="welcome-leftbar" ><div class="menu-title"><a href="http://ieeevis.org" target="_self">Welcome</a></div></div><div id="calendar-leftbar" class="open"><h3 class="menu-title" id="menu-title">Week-at-a-Glance</h3><ol class="link-leftbar"><li><a href="http://ieeevis.org/year/2012/calendar/2012-10-14">October 14, 2012</a></li><li><a href="http://ieeevis.org/year/2012/calendar/2012-10-15">October 15, 2012</a></li><li><a href="http://ieeevis.org/year/2012/calendar/2012-10-16">October 16, 2012</a></li><li><a href="http://ieeevis.org/year/2012/calendar/2012-10-17">October 17, 2012</a></li><li><a href="http://ieeevis.org/year/2012/calendar/2012-10-18">October 18, 2012</a></li><li><a href="http://ieeevis.org/year/2012/calendar/2012-10-19">October 19, 2012</a></li></ol></div><div id="session-leftbar" ><h3 class="menu-title" id="menu-title">Visweek Sessions</h3><ol class="link-leftbar"><li><a href="http://ieeevis.org/year/2012/keynote-session/all/all" target="_self">Keynote and Capstone</a></li><br /><li><a href="http://ieeevis.org/year/2012/paper-session/all/all" target="_self">Papers </a></li><li><a href="http://ieeevis.org/year/2012/paper-session/all/vis" target="_self">SciVis</a> &#183; <a href="http://ieeevis.org/year/2012/paper-session/all/infovis">InfoVis</a> &#183; <a href="http://ieeevis.org/year/2012/paper-session/all/vast">VAST</a> &#183; <a href="http://ieeevis.org/year/2012/paper-session/all/tvcg">TVCG</a></li><br /><li><a href="http://ieeevis.org/year/2012/poster-session/all/all" target="_self">Posters </a></li><br /><li><a href="http://ieeevis.org/year/2012/contest-session/all/all" target="_self">Contest & Challenge</a></li><li><a href="http://ieeevis.org/year/2012/contest-session/all/vis" target="_self">SciVis</a> &#183; <a href="http://ieeevis.org/year/2012/workshop/vast/vast-challenge">VAST</a></li><br /><li><a href="http://ieeevis.org/year/2012/panel-session/all/all" target="_self">Panels</a></li><br  /><li><a href="http://ieeevis.org/year/2012/workshop-session/all/all" target="_self">Workshops</a></li><br  /><li><a href="http://ieeevis.org/year/2012/tutorial-session/all/all" target="_self">Tutorials</a></li><br  /><li><a href="http://ieeevis.org/year/2012/public-session/all/all" target="_self">VisWeek Special Sessions</a></li><br  /><li><a href="http://visweek.org/attachments/2012_VisWeekArtShow.pdf" target="_self">Art Show Catalog</a></li><br  /><li><a href="http://ieeevis.org/year/2012/bof-session/all/all" target="_self">BOF meetings</a></li><br  /></ol></div><div class="menu-title"><a href="http://ieeevis.org/year/2012/info/overview-amp-topics/latest-news">Latest News</a><br  /></div><div class="menu-title"><a href="http://ieeevis.org/year/2012/info/exhibition/supporters-and-exhibition">Supporters and Exhibition</a><br  /></div><div class="menu-title"><a href="http://ieeevis.org/year/2012/info/registration/conference-registration">Conference Registration</a><br  /></div><div class="menu-title"><a href="http://ieeevis.org/year/2012/info/volunteer/visweek-compass-2012">Compass</a><br  /></div><div id="registration-leftbar" class="open"><h3 class="menu-title" id="menu-title">Travel and Hotel</h3><ol class="link-leftbar"><li><a href="http://ieeevis.org/year/2012/info/registration/hotel-reservations">Hotel Reservations</a></li><li><a href="http://ieeevis.org/year/2012/info/registration/getting-around-seattle">Getting around Seattle</a></li><li><a href="http://ieeevis.org/year/2012/info/registration/visa-assistance">Visa Assistance</a></li></ol></div><div class="menu-title"><a href="http://ieeevis.org/year/2012/info/volunteer/student-volunteer-information">Student Volunteers</a><br  /></div><div id="presenter-leftbar" class="open"><h3 class="menu-title" id="menu-title">Participant Information</h3><ol class="link-leftbar"><li><a href="http://ieeevis.org/year/2012/info/presenter-information/authors">Authors</a></li><li><a href="http://ieeevis.org/year/2012/info/presenter-information/poster-presenters">Poster Presenters</a></li><li><a href="http://ieeevis.org/year/2012/info/presenter-information/session-chairs">Session Chairs</a></li></ol></div><div id="cfp-leftbar" class="_blank"><h3 class="menu-title" id="menu-title">Call for Participation</h3><ol class="link-leftbar"><li>Papers</li><li><a href="http://ieeevis.org/year/2012/info/call-participation/scivis-papers" target="_self">SciVis</a>  &#183; <a href="http://ieeevis.org/year/2012/info/call-participation/infovis-papers">InfoVis</a>  &#183; <a href="http://ieeevis.org/year/2012/info/call-participation/vast-papers">VAST</a></li><br /><li><a href="http://ieeevis.org/year/2012/info/call-participation/posters" target="_self">Posters</a></li><br  /><li>Contests & Challenge</li><li><a href="http://ieeevis.org/year/2012/info/call-participation/scivis-contest">SciVis</a> &#183; <a href="http://ieeevis.org/year/2012/info/call-participation/vast-challenge">VAST</a> &#183; <a href="http://www.biovis.net/biovis/2012/info/contest" target="_blank">BioVis</a> &#183; <a href="http://ldav.org/viscontest.html" target="_blank">LDAV</a></li><br /><li><a href="http://ieeevis.org/year/2012/info/call-participation/tutorials" target="_self">Tutorials</a></li><br  /><li><a href="http://ieeevis.org/year/2012/info/call-participation/workshops" target="_self">Workshops</a></li><br  /><li><a href="http://ieeevis.org/year/2012/info/call-participation/panels" target="_self">Panels</a></li><br  /><li><a href="http://ieeevis.org/year/2012/info/call-participation/art-show" target="_self">Art Show</a></li><br  /><li><a href="http://ieeevis.org/year/2012/info/call-participation/doctoral-colloquium" target="_self">Doctoral Colloquium</a></li><br  /><li><a href="http://ieeevis.org/year/2012/info/call-participation/bof-meetings" target="_self">BOF meetings</a></li><br  /><li><a href="http://ieeevis.org/year/2012/info/call-participation/industry-involvement" target="_self">Industry Track</a></li><br  /></ol></div><div id="committee-leftbar"><h3 class="menu-title" id="menu-title">Co-located Symposia</h3><ol class="link-leftbar"><li><a href="http://www.ldav.org" target="_blank">IEEE LDAV 2012</a></li><br /><li><a href="http://www.biovis.net" target="_blank">IEEE BioVis 2012</a></li><br /></ol></div><div id="committee-leftbar" class="open"><h3 class="menu-title" id="menu-title">Committees</h3><ol class="link-leftbar"><li><a href="http://ieeevis.org/year/2012/info/committees/conference-committee" target="_self">Conference Committee</a></li><br /><li>Program Committees</li><li><a href="http://ieeevis.org/year/2012/info/committees/scivis-program-committee" target="_self">SciVis</a> &#183; <a href="http://ieeevis.org/year/2012/info/committees/infovis-program-committee">InfoVis</a> &#183; <a href="http://ieeevis.org/year/2012/info/committees/vast-program-committee">VAST</a></li><br /><li>Steering Committees</li><li><a href="http://ieeevis.org/year/2012/info/committees/scivis-steering-committee" target="_self">Vis</a> &#183; <a href="http://ieeevis.org/year/2012/info/committees/infovis-steering-committee">InfoVis</a> &#183; <a href="http://ieeevis.org/year/2012/info/committees/vast-steering-committee">VAST</a></li><br /></ol></div><div class="menu-title"><a href="mailto:info@visweek.org">Email Us</a></div><div id="archive-leftbar" class="open"><h3 id="menu-title" class="menu-title">Previous Years</h3><ol class="link-leftbar"><li><a href="http://ieeevis.org/year/2011/info/call-participation/welcome">2011</a>  &#183;<a href="http://vis.computer.org/VisWeek2010/">2010</a> &#183; <a href="http://vis.computer.org/VisWeek2009/">2009</a> &#183; <a href="http://vis.computer.org/VisWeek2008/">2008</a> &#183;</li> <li><a href="http://vis.computer.org/vis2007/">2007</a> &#183;<a href="http://vis.computer.org/vis2006/">2006</a> &#183; <a href="http://vis.computer.org/vis2005/">2005</a> &#183; <a href="http://vis.computer.org/vis2004/">2004</a> &#183;</li> <li><a href="http://vis.computer.org/vis2003/">2003</a> &#183;<a href="http://vis.computer.org/vis2002/">2002</a> &#183; <a href="http://vis.computer.org/vis2001/">2001</a> &#183; <a href="http://www.hpc.msstate.edu/conferences/vis00/">2000</a></li><li><a href="http://www.hpc.msstate.edu/conferences/vis99/">1999</a></li></ol></div></div><br /></div>
</div>
        </div>
            <div id="center"><div id="squeeze"><div class="right-corner"><div class="left-corner">
          <div class="breadcrumb"><a href="http://ieeevis.org/year/2016/info/vis-welcome/welcome">Home</a></div>                                <h2>2012 IEEE VAST Papers </h2>                                                  <div class="clear-block">
                        <div class="view view-paper-session view-id-paper_session view-display-id-page_1 view-dom-id-1">
    
  
  
      <div class="view-content">
      <div class="item-list views-accordion views-accordion-paper_session-page_1">
      <span class="session-event-title views-accordion-paper_session-page_1">VAST Papers: Text and Categorical Data Analysis</span>
    <div id="views-accordion-paper_session-page_1">
                <span class="label">Session :&nbsp;</span><div class="session-title">Text and Categorical Data Analysis</div><span class="label">Date & Time :&nbsp;</span>October 16 02:00 pm - 03:40 pm<br /><span class="label">Location :&nbsp;</span>Grand Ballroom B<br /><span class="label">Chair :&nbsp;</span>David Ebert<br /><span class="label">Papers :&nbsp;</span>      <div class="views-accordion-item accordion-item-0 accordion-item-odd accordion-item-first titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Reinventing the Contingency Wheel: Scalable Visual Analytics of Large Categorical Data </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/reinventing-contingency-wheel-scalable-visual-analytics-large-categorical-data"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1434?destination=node/1434"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Bilal Alsallakh, Wolfgang Aigner, Silvia Miksch, Eduard Groller </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />Contingency tables summarize the relations between categorical variables and  
arise in both scientific and business domains. Asymmetrically large two-way  
contingency tables pose a problem for common visualization methods. The  
Contingency Wheel has been recently proposed as an interactive visual method  
to explore and analyze such tables. However, the scalability and readability  
of this method are limited when dealing with large and dense tables. In this  
paper we present Contingency Wheel++, new visual analytics methods that  
overcome these major shortcomings: (1) regarding automated methods, a measure  
of association based on Pearson's residuals alleviates the bias of the raw  
residuals originally used, (2) regarding visualization methods, a  
frequency-based abstraction of the visual elements eliminates overlapping and  
makes analyzing both positive and negative associations possible, and (3)  
regarding the interactive exploration environment, a multi-level  
overview+detail interface enables exploring individual data items that are  
aggregated in the visualization or in the table using coordinated views. We  
illustrate the applicability of these new methods with a use case and show  
how they enable discovering and analyzing nontrivial patterns and  
associations in large categorical data
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-1 accordion-item-even titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Visual Classifier Training for Text Document Retrieval </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/visual-classifier-training-text-document-retrieval"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1438?destination=node/1438"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Florian Heimerl, Steffen Koch, Harald Bosch, Thomas Ertl </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />Performing exhaustive searches over a large number of text documents can be  
tedious, since it is very hard to formulate search queries or define filter  
criteria that capture an analyst's information need adequately.  
Classification through machine learning has the potential to improve search  
and filter tasks encompassing either complex or very specific information  
needs, individually. Unfortunately, analysts who are knowledgeable in their  
field are typically not machine learning specialists. Most classification  
methods, however, require a certain expertise regarding their parametrization  
to achieve good results. Supervised machine learning algorithms, in contrast,  
rely on labeled data, which can be provided by analysts. However, the effort  
for labeling can be very high, which shifts the problem from composing  
complex queries or defining accurate filters to another laborious task, in  
addition to the need for judging the trained classifier's quality. We  
therefore compare three approaches for interactive classifier training in a  
user study. All of the approaches are potential candidates for the  
integration into a larger retrieval system. They incorporate active learning  
to various degrees in order to reduce the labeling effort as well as to  
increase effectiveness. Two of them encompass interactive visualization for  
letting users explore the status of the classifier in context of the labeled  
documents, as well as for judging the quality of the classifier in iterative  
feedback loops. We see our work as a step towards introducing user controlled  
classification methods in addition to text search and filtering for  
increasing recall in analytics scenarios involving large corpora.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-2 accordion-item-odd titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Relative N-Gram Signatures: Document Visualization at the Level of Character N-Grams </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/relative-n-gram-signatures-document-visualization-level-character-n-grams"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1439?destination=node/1439"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Magdalena Jankowska, Vlado Keselj, Evangelos Milios </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />The Common N-Gram (CNG) classifier is a text classification algorithm based  
on the comparison of frequencies of character n-grams (strings of characters  
of length n) that are the most common in the considered documents and classes  
of documents. We present a text analytic visualization system that employs  
the CNG approach for text classification and uses the differences in  
frequency values of common n-grams in order to visually compare documents at  
the sub-word level. The visualization method provides both an insight into  
n-gram characteristics of documents or classes of documents and a visual  
interpretation of the workings of the CNG classifier.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-3 accordion-item-even titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->LeadLine: Interactive Visual Analysis of Text Data through Event Identification and Exploration </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/leadline-interactive-visual-analysis-text-data-through-event-identification-and"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1458?destination=node/1458"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Wenwen Dou, Xiaoyu Wang, Drew Skau, William Ribarsky, Michelle Zhou </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />Text data such as online news and microblogs bear valuable insights regarding  
important events and responses to such events. Events are inherently  
temporal, evolving over time. Existing visual text analysis systems have  
provided temporal views of changes based on topical themes extracted from  
text data. But few have associated topical themes with events that cause the  
changes. In this paper, we propose an interactive visual analytics system,  
LeadLine, to automatically identify meaningful events in news and social  
media data and support exploration of the events. To characterize events,  
LeadLine integrates topic modeling, event detection, and named entity  
recognition techniques to automatically extract information regarding the  
investigative 4 Ws: who, what, when, and where for each event. To further  
support analysis of the text corpora through events, LeadLine allows users to  
interactively examine meaningful events using the 4 Ws to develop an  
understanding of how and why. Through representing large-scale text corpora  
in the form of meaningful events, LeadLine provides a concise summary of the  
corpora. LeadLine also supports the construction of simple narratives through  
the exploration of events. To demonstrate the efficacy of LeadLine in  
identifying events and supporting exploration, two case studies were  
conducted using news and social media data.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-4 accordion-item-odd accordion-item-last titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->The Deshredder: A Visual Analytic Approach to Reconstructing Shredded Documents </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/deshredder-visual-analytic-approach-reconstructing-shredded-documents"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1465?destination=node/1465"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Patrick Butler, Prithwish Chakraborty, Naren Ramakrishnan </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />Reconstruction of shredded documents remains a significant challenge.  
Creating a better document reconstruction system enables not just recovery of  
information accidentally lost but also understanding our limitations against  
adversaries' attempts to gain access to information. Existing approaches to  
reconstructing shredded documents adopt either a predominantly manual (e.g.,  
crowd-sourcing) or a near automatic approach. We describe  
\\textit{Deshredder}, a visual analytic approach that scales well and  
effectively incorporates user input to direct the reconstruction  
process.Deshredder represents shredded pieces as time series and uses nearest  
neighbor matching techniques that enable matching both the contours of  
shredded pieces as well as the content of shreds themselves. More  
importantly, Deshredder's interface support visual analytics through user  
interaction with similarity matrices as well as higher level assembly through  
more complex stitching functions. We identify a functional task taxonomy  
leading to design considerations for constructing deshredding solutions, and  
describe how Deshredder applies to problems from the DARPA Shredder Challenge  
through expert evaluations.
</div>
  </div>
</div>
      </div>
</div>
<div class="item-list views-accordion views-accordion-paper_session-page_1">
      <span class="session-event-title views-accordion-paper_session-page_1">VAST Papers: Clustering, Classification, and Correlation</span>
    <div id="views-accordion-paper_session-page_1">
                <span class="label">Session :&nbsp;</span><div class="session-title">Clustering, Classification, and Correlation</div><span class="label">Date & Time :&nbsp;</span>October 16 10:30 am - 12:10 pm<br /><span class="label">Location :&nbsp;</span>Grand Ballroom B<br /><span class="label">Chair :&nbsp;</span>Daniel Weiskopf<br /><span class="label">Papers :&nbsp;</span>      <div class="views-accordion-item accordion-item-0 accordion-item-odd accordion-item-first titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->A Correlative Analysis Process in a Visual Analytics Environment </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/correlative-analysis-process-visual-analytics-environment"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1437?destination=node/1437"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Abish Malik, Ross Maciejewski, Yun Jang, Whitney Huang, Niklas Elmqvist, David Ebert </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />Finding patterns and trends in spatial and temporal datasets has been a long  
studied problem in statistics and different domains of science. This paper  
presents a visual analytics approach for the interactive exploration and  
analysis of spatiotemporal correlations among multivariate datasets. Our  
approach enables users to discover correlations and explore potentially  
causal or predictive links at different spatiotemporal aggregation levels  
among the datasets, and allows them to understand the underlying statistical  
foundations that precede the analysis. Our technique utilizes the Pearson's  
product-moment correlation coefficient and factors in the lead or lag between  
different datasets to detect trends and periodic patterns amongst them.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-1 accordion-item-even titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Scatter/Gather Clustering: Flexibly Incorporating User Feedback to Steer Clustering Results </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/scattergather-clustering-flexibly-incorporating-user-feedback-steer-clustering-"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1441?destination=node/1441"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Mahmud Hossain, Praveen Ojili, Cindy Grimm, Rolf Muller, Layne Watson, Naren Ramakrishnan </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />Significant effort has been devoted to designing clustering algorithms that  
are responsive to user feedback or that incorporate prior domain knowledge in  
the form of constraints. However, users desire more expressive forms of  
interaction to influence clustering outcomes. In our experiences working with  
diverse application scientists, we have identified an interaction style  
scatter/gather clustering that helps users iteratively restructure clustering  
results to meet their expectations. As the names indicate, scatter and gather  
are dual primitives that describe whether clusters in a current segmentation  
should be broken up further or, alternatively, brought back together. By  
combining scatter and gather operations in a single step, we support very  
expressive dynamic restructurings of data. Scatter/gather clustering is  
implemented using a nonlinear optimization framework that achieves both  
locality of clusters and satisfaction of user-supplied constraints. We  
illustrate the use of our scatter/gather clustering approach in a visual  
analytic application to study baffle shapes in the bat biosonar (ears and  
nose) system. We demonstrate how domain experts are adept at supplying  
scatter/gather constraints, and how our framework incorporates these  
constraints effectively without requiring numerous instance-level  
constraints.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-2 accordion-item-odd titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Inter-Active Learning of Ad-Hoc Classifiers for Video Visual Analytics </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/inter-active-learning-ad-hoc-classifiers-video-visual-analytics"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1443?destination=node/1443"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Benjamin Hoferlin, Rudolf Netzel, Markus Hoferlin, Daniel Weiskopf, Gunther Heidemann </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />Learning of classifiers to be used as filters within the analytical reasoning  
process leads to new and aggravates existing challenges. Such classifiers are  
typically trained ad-hoc, with tight time constraints that affect the amount  
and the quality of annotation data and, thus, also the users' trust in the  
classifier trained. We approach the challenges of ad-hoc training by  
inter-active learning, which extends active learning by integrating human  
experts' background knowledge to greater extent. In contrast to active  
learning, not only does inter-active learning include the users' expertise by  
posing queries of data instances for labeling, but it also supports the users  
in comprehending the classifier model by visualization. Besides the  
annotation of manually or automatically selected data instances, users are  
empowered to directly adjust complex classifier models. Therefore, our model  
visualization facilitates the detection and correction of inconsistencies  
between the classifier model trained by examples and the user's mental model  
of the class definition. Visual feedback of the training process helps the  
users assess the performance of the classifier and, thus, build up trust in  
the filter created. We demonstrate the capabilities of inter-active learning  
in the domain of video visual analytics and compare its performance with the  
results of random sampling and uncertainty sampling of training sets.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-3 accordion-item-even titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Visual Cluster Exploration of Web Clickstream Data </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/visual-cluster-exploration-web-clickstream-data"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1454?destination=node/1454"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Jishang Wei, Zeqian Shen, Neel Sundaresan, Kwan-Liu Ma </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />Web clickstream data are routinely collected to study how users browse the  
web or use a service. It is clear that the ability to recognize and summarize  
user behavior patterns from such data is valuable to e-commerce companies. In  
this paper, we introduce a visual analytics system to explore the various  
user behavior patterns reflected by distinct clickstream clusters. In a  
practical analysis scenario, the system first presents an overview of  
clickstream clusters using a Self-Organizing Map with Markov chain models.  
Then the analyst can interactively explore the clusters through an intuitive  
user interface. He can either obtain summarization of a selected group of  
data or further refine the clustering result. We evaluated our system using  
two different datasets from eBay. Analysts who were working on the same data  
have confirmed the system's effectiveness in extracting user behavior  
patterns from complex datasets and enhancing their ability to reason.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-4 accordion-item-odd accordion-item-last titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->An Adaptive Parameter Space-Filling Algorithm for Highly Interactive Cluster Exploration </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/adaptive-parameter-space-filling-algorithm-highly-interactive-cluster-explorati"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1468?destination=node/1468"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Zafar Ahmed, Chris Weaver </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />For a user to perceive continuous interactive response time in a  
visualization tool, the rule of thumb is that it must process, deliver, and  
display rendered results for any given interaction in under 100 milliseconds.  
In many visualization systems, successive interactions trigger independent  
queries and caching of results. Consequently, computationally expensive  
queries like multidimensional clustering cannot keep up with rapid sequences  
of interactions, precluding visual benefits such as motion parallax. In this  
paper, we describe a heuristic prefetching technique to improve the  
interactive response time of KMeans clustering in dynamic query  
visualizations of multidimensional data. We address the tradeoff between high  
interaction and intense query computation by observing how related  
interactions on overlapping data subsets produce similar clustering results,  
and characterizing these similarities within a parameter space of  
interaction. We focus on the two-dimensional parameter space defined by the  
minimum and maximum values of a time range manipulated by dragging and  
stretching a one-dimensional filtering lens over a plot of time series data.  
Using calculation of nearest neighbors of interaction points in parameter  
space, we reuse partial query results from prior interaction sequences to  
calculate both an immediate best-effort clustering result and to schedule  
calculation of an exact result. The method adapts to user interaction  
patterns in the parameter space by reprioritizing the interaction neighbors  
of visited points in the parameter space. A performance study on Mesonet  
meteorological data demonstrates that the method is a significant improvement  
over the baseline scheme in which interaction triggers on-demand, exact-range  
clustering with LRU caching. We also present initial evidence that  
approximate, temporary clustering results are sufficiently accurate (compared  
to exact results) to convey useful cluster structure during rapid and  
protracted interaction.
</div>
  </div>
</div>
      </div>
</div>
<div class="item-list views-accordion views-accordion-paper_session-page_1">
      <span class="session-event-title views-accordion-paper_session-page_1">VAST Papers: Visual-Computational Analysis of Multivariate Data</span>
    <div id="views-accordion-paper_session-page_1">
                <span class="label">Session :&nbsp;</span><div class="session-title">Visual-Computational Analysis of Multivariate Data</div><span class="label">Date & Time :&nbsp;</span>October 16 04:15 pm - 05:55 pm<br /><span class="label">Location :&nbsp;</span>Grand Ballroom B<br /><span class="label">Chair :&nbsp;</span>Jean-Daniel Fekete<br /><span class="label">Papers :&nbsp;</span>      <div class="views-accordion-item accordion-item-0 accordion-item-odd accordion-item-first titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Dis-Function: Learning Distance Functions Interactively </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/dis-function-learning-distance-functions-interactively"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1442?destination=node/1442"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Eli T. Brown, Jingjing Liu, Carla E. Brodley, Remco Chang </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />The world's corpora of data grow in size and complexity every day, making it  
increasingly difficult for experts to make sense out of their data. Although  
machine learning offers algorithms for finding patterns in data  
automatically, they often require algorithm-specific parameters, such as an  
appropriate distance function, which are outside the purview of a domain  
expert. We present a system that allows an expert to interact directly with a  
visual representation of the data to define an appropriate distance function,  
thus avoiding direct manipulation of obtuse model parameters. Adopting an  
iterative approach, our system first assumes a uniformly weighted Euclidean  
distance function and projects the data into a two-dimensional scatterplot  
view. The user can then move incorrectly-positioned data points to locations  
that reflect his or her understanding of the similarity of those data points  
relative to the other data points. Based on this input, the system performs  
an optimization to learn a new distance function and then re-projects the  
data to redraw the scatterplot. We illustrate empirically that with only a  
few iterations of interaction and optimization, a user can achieve a  
scatterplot view and its corresponding distance function that reflect the  
user's knowledge of the data. In addition, we evaluate our system to assess  
scalability in data size and data dimension, and show that our system is  
computationally efficient and can provide an interactive or near-interactive  
user experience.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-1 accordion-item-even titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Visual Pattern Discovery using Random Projections </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/visual-pattern-discovery-using-random-projections"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1451?destination=node/1451"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Anushka Anand, Leland Wilkinson, Tuan Nhon Dang </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />An essential element of exploratory data analysis is the use of revealing  
low-dimensional projections of high-dimensional data. Projection Pursuit has  
been an effective method for finding interesting low-dimensional projections  
of multidimensional spaces by optimizing a score function called a projection  
pursuit index. However, the technique is not scalable to high-dimensional  
spaces. Here, we introduce a novel method for discovering noteworthy views of  
high-dimensional data spaces by using binning and random projections. We  
define score functions, akin to projection pursuit indices, that characterize  
visual patterns of the low-dimensional projections that constitute feature  
subspaces. We also describe an analytic, multivariate visualization platform  
based on this algorithm that is scalable to extremely large problems.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-2 accordion-item-odd titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->iLAMP: Exploring High-Dimensional Spacing Through Backward Multidimensional Projection </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/ilamp-exploring-high-dimensional-spacing-through-backward-multidimensional-proj"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1453?destination=node/1453"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Elisa Portes dos Santos Amorim, Emilio Vital Brazil, Joel Daniel II, Paulo Joia, Luis Gustavo Nonato </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />Ever improving computing power and technological advances are greatly  
augmenting data collection and scientific observation. This has directly  
contributed to increased data complexity and dimensionality, motivating  
research of exploration techniques for multidimensional data. Consequently, a  
recent influx of work dedicated to techniques and tools that aid in  
understanding multidimensional datasets can be observed in many research  
fields, including biology, engineering, physics and scientific computing.  
While the effectiveness of existing techniques to analyze the structure and  
relationships of multidimensional data varies greatly, few techniques provide  
flexible mechanisms to simultaneously visualize and actively explore  
high-dimensional spaces. In this paper, we present an inverse linear affine  
multidimensional projection, coined iLAMP, that enables a novel interactive  
exploration technique for multidimensional data. iLAMP operates in reverse to  
traditional projection methods by mapping low-dimensional information into a  
high-dimensional space. This allows users to extrapolate instances of a  
multidimensional dataset while exploring a projection of the data to the  
planar domain. We present experimental results that validate iLAMP, measuring  
the quality and coherence of the extrapolated data; as well as demonstrate  
the utility of iLAMP to hypothesize the unexplored regions of a  
high-dimensional space.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-3 accordion-item-even titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Subspace Search and Visualization to Make Sense of Alternative Clusterings in High-Dimensional Data </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/subspace-search-and-visualization-make-sense-alternative-clusterings-high-dimen"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1456?destination=node/1456"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Andrada Tatu, Fabian Maas, Ines Farber, Enrico Bertini, Tobias Schreck, Thomas Seidl, Daniel Keim </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />In explorative data analysis, the data under consideration often resides in a  
high-dimensional (HD) data space. Currently many methods are available to  
analyze this type of data. So far, proposed automatic approaches include  
dimensionality reduction and cluster analysis, whereby visual-interactive  
methods aim to provide effective visual mappings to show, relate, and  
navigate HD data. Furthermore, almost all of these methods conduct the  
analysis from a singular perspective, meaning that they consider the data in  
either the original HD data space, or a reduced version thereof.  
Additionally, HD data spaces often consist of combined features that measure  
different properties, in which case the particular relationships between the  
various properties may not be clear to the analysts a priori since it can  
only be revealed if appropriate feature combinations (subspaces) of the data  
are taken into consideration. Considering just a single subspace is, however,  
often not sufficient since different subspaces may show complementary,  
conjointly, or contradicting relations between data items. Useful information  
may consequently remain embedded in sets of subspaces of a given HD input  
data space. Relying on the notion of subspaces, we propose a novel method for  
the visual analysis of HD data in which we employ an interestingness-guided  
subspace search algorithm to detect a candidate set of subspaces. Based on  
appropriately defined subspace similarity functions, we visualize the  
subspaces and provide navigation facilities to interactively explore large  
sets of subspaces. Our approach allows users to effectively compare and  
relate subspaces with respect to involved dimensions and clusters of objects.  
We apply our approach to synthetic and real data sets. We thereby demonstrate  
its support for understanding HD data from different perspectives,  
effectively yielding a more complete view on HD data.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-4 accordion-item-odd accordion-item-last titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Just-in-Time Annotation of Clusters, Outliers, and Trends in Point-based Data Visualizations </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/just-time-annotation-clusters-outliers-and-trends-point-based-data-visualizatio"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1464?destination=node/1464"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Eser Kandogan </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />We introduce the concept of just-in-time descriptive analytics as a novel  
application of computational and statistical techniques performed at  
interaction-time to help users easily understand the structure of data as  
seen in visualizations. Fundamental to just-in-time descriptive analytics is  
(a) identifying visual features, such as clusters, outliers, and trends, user  
might observe in visualizations automatically, (b) determining the semantics  
of such features by performing statistical analysis as the user is  
interacting, and (c) enriching visualizations with annotations that not only  
describe semantics of visual features but also facilitate interaction to  
support high-level understanding of data. In this paper, we demonstrate  
just-in-time descriptive analytics applied to a point-based multi-dimensional  
visualization technique to identify and describe clusters, outliers, and  
trends. We argue that it provides a novel user experience of computational  
techniques working alongside of users allowing them to build faster  
qualitative mental models of data by demonstrating its application on a few  
use-cases. Techniques used to facilitate just-in-time descriptive analytics  
are described in detail along with their run-time performance  
characteristics. We believe this is just a starting point and much remains to  
be researched, as we discuss open issues and opportunities in improving  
accessibility and collaboration.
</div>
  </div>
</div>
      </div>
</div>
<div class="item-list views-accordion views-accordion-paper_session-page_1">
      <span class="session-event-title views-accordion-paper_session-page_1">VAST Papers: Sensemaking and Collaboration</span>
    <div id="views-accordion-paper_session-page_1">
                <span class="label">Session :&nbsp;</span><div class="session-title">Sensemaking and Collaboration</div><span class="label">Date & Time :&nbsp;</span>October 17 08:30 am - 10:10 am<br /><span class="label">Location :&nbsp;</span>Grand Ballroom B<br /><span class="label">Chair :&nbsp;</span>Niklas Elmqvist<br /><span class="label">Papers :&nbsp;</span>      <div class="views-accordion-item accordion-item-0 accordion-item-odd accordion-item-first titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Examining the Use of a Visual Analytics System for Sensemaking Tasks: Case Studies with Domain Experts </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/examining-use-visual-analytics-system-sensemaking-tasks-case-studies-domain-exp"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1444?destination=node/1444"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Youn-ah Kang, John Stasko </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />While the formal evaluation of systems in visual analytics is still  
relatively uncommon, particularly rare are case studies of prolonged system  
use by domain analysts working with their own data. Conducting case studies  
can be challenging, but it can be a particularly effective way to examine  
whether visual analytics systems are truly helping expert users to accomplish  
their goals. We studied the use of a visual analytics system for sensemaking  
tasks on documents by six analysts from a variety of domains. We describe  
their application of the system along with the benefits, issues, and problems  
that we uncovered. Findings from the studies identify features that visual  
analytics systems should emphasize as well as missing capabilities that  
should be addre ssed. These findings inform design implications for future  
systems.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-1 accordion-item-even titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Semantic Interaction for Sensemaking: Inferring Analytical Reasoning for Model Steering </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/semantic-interaction-sensemaking-inferring-analytical-reasoning-model-steering"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1445?destination=node/1445"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Alex Endert, Patrick Fiaux, Chris North </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />Visual analytic tools aim to support the cognitively demanding task of  
sensemaking. Their success often depends on the ability to leverage  
capabilities of mathematical models, visualization, and human intuition  
through flexible, usable, and expressive interactions. Spatially clustering  
data is one effective metaphor for users to explore similarity and  
relationships between information, adjusting the weighting of dimensions or  
characteristics of the dataset to observe the change in the spatial layout.  
Semantic interaction is an approach to user interaction in such  
spatializations that couples these parametric modifications of the clustering  
model with users' analytic operations on the data (e.g., direct document  
movement in the spatialization, highlighting text, search, etc.). In this  
paper, we present results of a user study exploring the ability of semantic  
interaction in a visual analytic prototype, ForceSPIRE, to support  
sensemaking. We found that semantic interaction captures the analytical  
reasoning of the user through keyword weighting, and aids the user in  
co-creating a spatialization based on the user's reasoning and intuition.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-2 accordion-item-odd titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->SocialNetSense: Supporting Sensemaking of Social and Structural Features in Networks with Interactive Visualization </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/socialnetsense-supporting-sensemaking-social-and-structural-features-networks-i"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1452?destination=node/1452"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Liang Gou, Xiaolong (Luke) Zhang, Airong Luo, Patricia F Anderson </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />Increasingly, social network datasets contain social attribute information  
about actors and their relationship. Analyzing such network with social  
attributes requires making sense of not only its structural features, but  
also the relationship between social features in attributes and network  
structures. Existing social network analysis tools are usually weak in  
supporting complex analytical tasks involving both structural and social  
features, and often overlook users' needs for sensemaking tools that help to  
gather, synthesize, and organize information of these features. To address  
these challenges, we propose a sensemaking framework of social-network visual  
analytics in this paper. This framework considers both bottom-up processes,  
which are about constructing new understandings based on collected  
information, and top-down processes, which concern using prior knowledge to  
guide information collection, in analyzing social networks from both social  
and structural perspectives. The framework also emphasizes the  
externalization of sensemaking processes through interactive visualization.  
Guided by the framework, we develop a system, SocialNetSense, to support the  
sensemaking in visual analytics of social networks with social attributes.  
The example of using our system to analyze a scholar collaboration network  
shows that our approach can help users gain insight into social networks both  
structurally and socially, and enhance their process awareness in visual  
analytics.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-3 accordion-item-even titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->An Affordance-Based Framework for Human Computation and Human-Computer Collaboration </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/affordance-based-framework-human-computation-and-human-computer-collaboration"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1457?destination=node/1457"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />R. Jordan Crouser, Remco Chang </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"></div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-4 accordion-item-odd accordion-item-last titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Analyst&#039;s Workspace: An Embodied Sensemaking Environment for Large, High Resolution Displays </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/analysts-workspace-embodied-sensemaking-environment-large-high-resolution-displ"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1463?destination=node/1463"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Christopher Andrews, Chris North </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />Distributed cognition and embodiment provide compelling models for how humans  
think and interact with the environment. Our examination of the use of large,  
high-resolution displays from an embodied perspective has lead directly to  
the development of a new sensemaking environment called Analyst's Workspace  
(AW). AW leverages the embodied resources made more accessible through the  
physical nature of the display to create a spatial workspace. By combining  
spatial layout of documents and other artifacts with an entity-centric,  
explorative investigative approach, AW aims to allow the analyst to  
externalize elements of the sensemaking process as a part of the  
investigation, integrated into the visual representations of the data itself.  
In this paper, we describe the various capabilities of AW and discuss the key  
principles and concepts underlying its design, emphasizing unique design  
principles for designing visual analytic tools for large, high-resolution  
displays.
</div>
  </div>
</div>
      </div>
</div>
<div class="item-list views-accordion views-accordion-paper_session-page_1">
      <span class="session-event-title views-accordion-paper_session-page_1">VAST Papers: Applications, Design Studies and Tools</span>
    <div id="views-accordion-paper_session-page_1">
                <span class="label">Session :&nbsp;</span><div class="session-title">Applications, Design Studies and Tools</div><span class="label">Date & Time :&nbsp;</span>October 18 10:30 am - 12:10 pm<br /><span class="label">Location :&nbsp;</span>Grand Ballroom B<br /><span class="label">Chair :&nbsp;</span>Enrico Bertini<br /><span class="label">Papers :&nbsp;</span>      <div class="views-accordion-item accordion-item-0 accordion-item-odd accordion-item-first titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Visual Analytics Methodology for Eye Movement Studies </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/visual-analytics-methodology-eye-movement-studies"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1435?destination=node/1435"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Gennady Andrienko, Natalia Andrienko, Michael Burch, Daniel Weiskopf </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />Eye movement analysis is gaining popularity as a tool for evaluation of  
visual displays and interfaces. However, the existing methods and tools for  
analyzing eye movements and scanpaths are limited in terms of the tasks they  
can support and effectiveness for large data and data with high variation. We  
have performed an extensive empirical evaluation of a broad range of visual  
analytics methods used in analysis of geographic movement data. The methods  
have been tested for the applicability to eye tracking data and the  
capability to extract useful knowledge about users' viewing behaviors. This  
allowed us to select the suitable methods and match them to possible analysis  
tasks they can support. The paper describes how the methods work in  
application to eye tracking data and provides guidelines for method selection  
depending on the analysis tasks
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-1 accordion-item-even titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->AlVis: Situation Awareness in the Surveillance of Road Tunnels </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/alvis-situation-awareness-surveillance-road-tunnels"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1446?destination=node/1446"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Harald Piringer, Matthias Buchetics, Rudolf Benedik </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />In the surveillance of road tunnels, video data plays an important role for a  
detailed inspection and as an input to systems for an automated detection of  
incidents. In disaster scenarios like major accidents, however, the increased  
amount of detected incidents may lead to situations where human operators  
lose a sense of the overall meaning of that data, a problem commonly known as  
a lack of situation awareness. The primary contribution of this paper is a  
design study of AlVis, a system designed to increase situation awareness in  
the surveillance of road tunnels. The design of AlVis is based on a  
simplified tunnel model which enables an overview of the spatio-temporal  
development of scenarios in real-time. The visualization explicitly  
represents the present state, the history, and predictions of potential  
future developments. Concepts for situation-sensitive prioritization of  
information ensure scalability from normal operation to major disaster  
scenarios. The visualization enables an intuitive access to live and historic  
video for any point in time and space. We illustrate AlVis by means of a  
scenario and report qualitative feedback by tunnel experts and operators.  
This feedback suggests that AlVis is suitable to save time in recognizing  
dangerous situations and helps to maintain an overview in complex disaster  
scenarios.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-2 accordion-item-odd titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Spatiotemporal Social Media Analytics for Abnormal Event Detection using Seasonal-Trend Decomposition </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/spatiotemporal-social-media-analytics-abnormal-event-detection-using-seasonal-t"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1447?destination=node/1447"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Junghoon Chae, Dennis Thom, Harald Bosch, Yun Jang, Ross Maciejewski, David S. Ebert, Thomas Ertl </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />Recent advances in technology have enabled social media services to support  
space-time indexed data, and internet users from all over the world have  
created a large volume of time-stamped, geo-located data. Such spatiotemporal  
data has immense value for increasing situational awareness of local events,  
providing insights for investigations and understanding the extent of  
incidents, their severity, and consequences, as well as their time-evolving  
nature. In analyzing social media data, researchers have mainly focused on  
finding temporal trends according to volume-based importance. Hence, a  
relatively small volume of relevant messages may easily be obscured by a huge  
data set indicating normal situations. In this paper, we present a visual  
analytics approach that provides users with scalable and interactive social  
media data analysis and visualization including the exploration and  
examination of abnormal topics and events within various social media data  
sources, such as Twitter, Flickr and YouTube. In order to find and understand  
abnormal events, the analyst can first extract major topics from a set of  
selected messages and rank them probabilistically using Latent Dirichlet  
Allocation. He can then apply seasonal trend decomposition together with  
traditional control chart methods to find unusual peaks and outliers within  
topic time series. Our case studies show that situational awareness can be  
improved by incorporating the anomaly and trend examination techniques into a  
highly interactive visual analysis process.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-3 accordion-item-even titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Visual Analytics for the Big Data Era -- A Comparative Review of State-of-the-Art Commercial Systems </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/visual-analytics-big-data-era-comparative-review-state-art-commercial-systems"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1449?destination=node/1449"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Leishi zhang, Andreas Stoffel, Michael Behrisch, Sebastian Mittelstadt, Tobias Schreck, Rene Pompl,  </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"></div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-4 accordion-item-odd accordion-item-last titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Smart Super Views - A Knowledge-Assisted Interface for Medical Visualization </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/smart-super-views-knowledge-assisted-interface-medical-visualization"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1466?destination=node/1466"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Gabriel Mistelbauer, Hamed Bouzari, Rudiger Schernthaner, Ivan Baclija, Arnold Kochl, Stefan Bruckne </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />Due to the ever growing volume of acquired data and information, users have  
to be constantly aware of the methods for their exploration and for  
interaction. Of these, not each might be applicable to the data at hand or  
might reveal the desired result. Owing to this, innovations may be used  
inappropriately and users may become skeptical. In this paper we propose a  
knowledge-assisted interface for medical visualization, which reduces the  
necessary effort to use new visualization methods, by providing only the most  
relevant ones in a smart way. Consequently, we are able to expand such a  
system with innovations without the users to worry about when, where, and  
especially how they may or should use them. We present an application of our  
system in the medical domain and give qualitative feedback from domain  
experts.
</div>
  </div>
</div>
      </div>
</div>
<div class="item-list views-accordion views-accordion-paper_session-page_1">
      <span class="session-event-title views-accordion-paper_session-page_1">VAST Papers: Space and Time, and The Analysis Process</span>
    <div id="views-accordion-paper_session-page_1">
                <span class="label">Session :&nbsp;</span><div class="session-title">Space and Time, and The Analysis Process</div><span class="label">Date & Time :&nbsp;</span>October 18 08:30 am - 10:10 am<br /><span class="label">Location :&nbsp;</span>Grand Ballroom B<br /><span class="label">Chair :&nbsp;</span>Bill Ribarsky<br /><span class="label">Papers :&nbsp;</span>      <div class="views-accordion-item accordion-item-0 accordion-item-odd accordion-item-first titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->The User Puzzle - Explaining the Interaction with Visual Analytics Systems </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/user-puzzle-explaining-interaction-visual-analytics-systems"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1436?destination=node/1436"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Margit Pohl, Michael Smuc, Eva Mayr </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />Visual analytics emphasizes the interplay between visualization, analytical  
procedures performed by computers and human perceptual and cognitive  
activities. Human reasoning is an important element in this context. There  
are several theories in psychology and HCI explaining open-ended and  
exploratory reasoning. Five of these theories (sensemaking theories, gestalt  
theories, distributed cognition, graph comprehension theories and  
skill-rule-knowledge models) are described in this paper. We discuss their  
relevance for visual analytics. In order to do this more systematically, we  
developed a schema of categories relevant for visual analytics research and  
evaluation. All these theories have strengths but also weaknesses in  
explaining interaction with visual analytics systems. A possibility to  
overcome the weaknesses would be to combine two or more of these theories.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-1 accordion-item-even titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Enterprise Data Analysis and Visualization: An Interview Study </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/enterprise-data-analysis-and-visualization-interview-study"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1448?destination=node/1448"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Sean Kandel, Andreas Paepcke, Joseph M. Hellerstein, Jeffrey Heer </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"></div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-2 accordion-item-odd titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Visual Analytics Methods for Categoric Spatio-Temporal Data </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/visual-analytics-methods-categoric-spatio-temporal-data"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1450?destination=node/1450"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Tatiana von Landesberger, Sebastian Bremm, Natalia Andrienko, Gennady Andrienko, Maria Tekusova </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />We focus on visual analysis of space- and time-referenced categorical data,  
which describe possible states of spatial (geographical) objects or locations  
and their changes over time. The analysis of these data is difficult as there  
are only limited possibilities to analyze the three aspects (location, time  
and category) simultaneously. We present a new approach which interactively  
combines (a) visualization of categorical changes over time; (b) various  
spatial data displays; (c) computational techniques for task-oriented  
selection of time steps. They provide an expressive visualization with regard  
to either the overall evolution over time or unusual changes. We apply our  
approach on two use cases demonstrating its usefulness for a wide variety of  
tasks. We analyze data from movement tracking and meteorologic areas. Using  
our approach, expected events could be detected and new insights were gained.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-3 accordion-item-even titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->A Visual Analytics Approach to Multi-scale Exploration of Environmental Time Series </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/visual-analytics-approach-multi-scale-exploration-environmental-time-series"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1455?destination=node/1455"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Mike Sips, Patrick Kothur, Andrea Unger, Christian Hege, Doris Dransch </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />We present a Visual Analytics approach that addresses the detection of  
interesting patterns in numerical time series, specifically from  
environmental sciences. Crucial for the detection of interesting temporal  
patterns are the time scale and the starting points one is looking at. Our  
approach makes no assumption about time scale and starting position of  
temporal patterns and consists of three main steps: an algorithm to compute  
statistical values for all possible time scales and starting positions of  
intervals, visual identification of potentially interesting patterns in a  
matrix visualization, and interactive exploration of detected patterns. We  
demonstrate the utility of this approach in two scientific scenarios and  
explain how it allowed scientists to gain new insight into the dynamics of  
environmental systems.
</div>
  </div>
</div>
              <div class="views-accordion-item accordion-item-4 accordion-item-odd accordion-item-last titles">  
  <div class="views-field-title">
                <span class="field-content"><div><!--Author: -->Watch This: A Taxonomy for Dynamic Data Visualization </div></span>
  </div>
  
  <div class="views-field-nid-1">
                <span class="field-content"></span>
  </div>
  
  <div class="views-field-nid">
                <span class="field-content"><div class="paper-links"><a title="Read entire document" href="http://ieeevis.org/year/2012/paper/vast/watch-taxonomy-dynamic-data-visualization"><img src="http://ieeevis.org/sites/all/themes/visweek/images/read_more.png"></a><a title="Get update notification" href="http://ieeevis.org/notifications/subscribe/1/thread/nid/1467?destination=node/1467"><img src="http://ieeevis.org/sites/all/themes/visweek/images/subscribe.png"></a></div></span>
  </div>
  
  <div class="views-field-field-paper-authors-value">
                <div class="field-content"><div><span class="label">Authors: </span><br />Joseph Cottam, Andrew Lumsdaine, Chris Weaver </div></div>
  </div>
  
  <div class="views-field-field-paper-abstract-value">
                <div class="field-content"><span class="label">Abstract : </span><br />Visualizations embody design choices about data access, data transformation,  
visual representation, and interaction. To interpret a static visualization,  
a person must identify the correspondences between the visual representation  
and the underlying data. These correspondences become moving targets when a  
visualization is dynamic. Dynamics may be introduced in a visualization at  
any point in the analysis and visualization process. For example, the data  
itself may be streaming, shifting subsets may be selected, visual  
representations may be animated, and interaction may modify presentation. In  
this paper, we focus on the impact of dynamic data. We present a taxonomy and  
conceptual framework for understanding how data changes influence the  
interpretability of visual representations. Visualization techniques are  
organized into categories at various levels of abstraction. The salient  
characteristics of each category and task suitability are discussed through  
examples from the scientific literature and popular practices. Examining the  
implications of dynamically updating visualizations warrants attention  
because it directly impacts the interpretability (and thus utility) of  
visualizations. The taxonomy presented provides a reference point for further  
exploration of dynamic data visualization techniques.
</div>
  </div>
</div>
      </div>
</div>
    </div>
  
    
  
  
  
</div>           </div>
                </div></div></div></div> <!-- /.left-corner, /.right-corner, /#squeeze, /#center -->
              <div id="sidebar-right" class="sidebar">
                    <div id="block-block-8" class="clear-block block block-block">
<!--
-->
  <div class="content"><div class="sidebar-title" id="important-dates">Important Dates</div>
<div class="event-title"><strike><strong>March 21st</strong><br><a	href="http://ieeevis.org/year/2016/info/call-participation/paper-submission-guidelines">Papers - abstract deadline</a></strike></div>
<div class="event-title"><strike><strong>March 31st</strong><br><a	href="http://ieeevis.org/year/2016/info/call-participation/paper-submission-guidelines">Papers - submission deadline</a></strike></div>
<div class="event-title"><strike><strong>April 30th</strong><br><a	href="http://ieeevis.org/year/2016/info/call-participation/tutorials">Tutorials - proposal submission deadline</a><br><a href="">Workshops - proposal submission deadline</a></strike></div>
<div class="event-title"><strike><strong>May 25th</strong><br><a	href="http://ieeevis.org/year/2016/info/call-participation/doctoral-colloquium">Doctoral Colloquium - submission deadline</a></strike></div>
<div class="event-title"><strike><strong>June 6th</strong><br><a	href="http://ieeevis.org/year/2016/info/call-participation/paper-submission-guidelines">Papers - notification of results of first review cycle</a></strike></div>
<div class="event-title"><strike><strong>June 12th</strong><br><a  href="http://www.vissv.org/visweektasksystem/htdocs/shirtcontest.html
">Student Volunteers - T-shirt design contest deadline</a></strike></div>
<div class="event-title"><strike><strong>June 15th</strong><br><a	href="http://ieeevis.org/year/2016/info/call-participation/panels">Panels - submission deadline</a></strike></div>
<div class="event-title"><strike><strong>June 24th</strong></strike><br><a	href="http://ieeevis.org/year/2016/info/call-participation/posters">Posters - submission deadline</a></strike></div>
<div class="event-title"><strike><strong>June 27th</strong><br><a	href="http://ieeevis.org/year/2016/info/call-participation/paper-submission-guidelines">Papers - submission deadline for second review cycle</a></strike></div>
<div class="event-title"><strike><strong>July 11th</strong><br><a	href="http://ieeevis.org/year/2016/info/call-participation/paper-submission-guidelines">Papers - final notification</a></strike></div>
<div class="event-title"><strike><strong>August 1st</strong><br><a	href="http://ieeevis.org/year/2016/info/call-participation/paper-submission-guidelines">Papers - camera-ready submission deadline</a>
<br><a href="http://www.vissv.org/visweektasksystem/htdocs/application.html">Student Volunteers - application deadline</a>
<br><a href="http://www.vissv.org/visweektasksystem/htdocs/tshirt.php">Student Volunteers - T-shirt design vote deadline</a>
</strike></div>
<div class="event-title"><strong>September 4th</strong><br><a	href="http://ieeevis.org/year/2016/info/call-participation/meetups">Meetups - proposal submission deadline</a></div>
<div class="event-title"><strong>September 9th</strong><br><a	href="#">VIS registration - early bird deadline</a></div>

</div>
</div>
<div id="block-block-6" class="clear-block block block-block">
<!--
-->
  <div class="content"><script src="https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min.js"></script><script>
function guessYear()
{
    // Tries to guess current year based on the HREF
    try {
        var year = window.location.pathname.split("/")[2];
        return Number(year) || 2016;
    } catch (e) {
        console.error("Could not guess year! Defaulting to 2016", window.location);
        return 2016;
    }
}

sponsorsJson = [
    {
        "class": "Platinum",
        "href": "http://www.kaust.edu.sa/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/logo_with_text_white.png",
        "year": 2011
    },
    {
        "class": "Platinum",
        "href": "http://www.microsoft.com/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/ms-logo_bL.png",
        "year": 2011
    },
    {
        "class": "Platinum",
        "href": "http://www.tableausoftware.com/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/Padded_Tableau_Logo.jpg",
        "year": 2011
    },
    {
        "class": "Gold",
        "href": "http://www.nlm.nih.gov/",
        "src": "/attachments\u2026pporterssites/visweek.vgtc.org/files/supporter/NLMLOGOBlueReproOutline.png",
        "year": 2011
    },
    {
        "class": "Gold",
        "href": "http://www.sci.utah.edu/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/SCI-logo-mono.png",
        "year": 2011
    },
    {
        "class": "Silver",
        "href": "http://www.research.ibm.com/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/ibm_high.gif",
        "year": 2011
    },
    {
        "class": "Silver",
        "href": "http://www.intel.com/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/intel.png",
        "year": 2011
    },
    {
        "class": "Silver",
        "href": "http://www.pnl.gov/computing/resources/nvac.stm",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/NVAC_DHS.jpg",
        "year": 2011
    },
    {
        "class": "Silver",
        "href": "http://www.pnl.gov/",
        "src": "/attachments\u2026porterssites/visweek.vgtc.org/files/supporter/PNNL_Color_Logo_Vertical.jpg",
        "year": 2011
    },
    {
        "class": "Silver",
        "href": "http://www.velir.com/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/VelirLogo_3-colorPMS.jpg",
        "year": 2011
    },
    {
        "class": "Publisher",
        "href": "http://www.akpeters.com/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/ak_peters.jpg",
        "year": 2011
    },
    {
        "class": "Publisher",
        "href": "http://www.morganclaypool.com",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/MCP_logo_02.jpg",
        "year": 2011
    },
    {
        "class": "Platinum Plus",
        "href": "http://www.ibm.com/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/IBM.jpeg",
        "year": 2012
    },
    {
        "class": "Platinum Plus",
        "href": "http://www.intel.com/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/intel_0.png",
        "year": 2012
    },
    {
        "class": "Platinum Plus",
        "href": "http://www.kaust.edu.sa/visweek",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/KAUST_0.png",
        "year": 2012
    },
    {
        "class": "Platinum Plus",
        "href": "http://www.microsoft.com/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/ms-logo_bL_0.png",
        "year": 2012
    },
    {
        "class": "Platinum Plus",
        "href": "http://www.tableausoftware.com/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/tableau_cmyk.png",
        "year": 2012
    },
    {
        "class": "Gold",
        "href": "http://www.pnnl.gov/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/PNNL.png",
        "year": 2012
    },
    {
        "class": "Silver",
        "href": "http://www.att.com/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/ATT_new_ai.png",
        "year": 2012
    },
    {
        "class": "Silver",
        "href": "http://www.battelle.org/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/Battelle%20Logo.jpg",
        "year": 2012
    },
    {
        "class": "Silver",
        "href": "http://www.igd.fraunhofer.de/en/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/IGD.png",
        "year": 2012
    },
    {
        "class": "Silver",
        "href": "http://www.infinitez.com/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/infinitez.png",
        "year": 2012
    },
    {
        "class": "Silver",
        "href": "http://www.nlm.nih.gov/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/NLM.png",
        "year": 2012
    },
    {
        "class": "Silver",
        "href": "http://www.sci.utah.edu/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/sci-logo-one-color.png",
        "year": 2012
    },
    {
        "class": "Silver",
        "href": "http://www.vacommunity.org/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/VACommunityLogo.png",
        "year": 2012
    },
    {
        "class": "Academic",
        "href": "http://www.purdue.edu/discoverypark/vaccine/index.php",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/vaccine_logo.png",
        "year": 2012
    },
    {
        "class": "Publisher",
        "href": "http://www.akpeters.com/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/akPeters.png",
        "year": 2012
    },
    {
        "class": "Publisher",
        "href": "http://www.morganclaypool.com/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/MCP.jpeg",
        "year": 2012
    },
    {
        "class": "Symposium Sponsor",
        "href": "http://www.agilent.com/",
        "src": "/attachments\u2026erssites/visweek.vgtc.org/files/supporter/Agilent_4c_CorporateSig-noHL.JPG",
        "year": 2012
    },
    {
        "class": "Symposium Sponsor",
        "href": "http://www.autodesk.com/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/Autodesk_logo.jpg",
        "year": 2012
    },
    {
        "class": "Symposium Sponsor",
        "href": "http://www.kitware.com/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/kitware.png",
        "year": 2012
    },
    {
        "class": "Symposium Sponsor",
        "href": "http://www.nature.com/nmeth/index.html",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/NMeth%20logo.png",
        "year": 2012
    },
    {
        "class": "Symposium Sponsor",
        "href": "http://www.sci.utah.edu/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/sci-logo-one-color_0.png",
        "year": 2012
    },
    {
        "class": "Symposium Sponsor",
        "href": "http://www.nationwidechildrens.org/battelle-center-for-mathematical-medicine",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/nationwide-childrens.png",
        "year": 2012
    },
    {
        "class": "Symposium Sponsor",
        "href": "http://energy.gov/",
        "src": "/attachments/supporterssites/visweek.vgtc.org/files/supporter/doe.png",
        "year": 2012
    },
    {
        "class": "Platinum",
        "href": "http://www.cisco.com/",
        "src": "/attachments/supporters/tmp/Cisco_Logo.png",
        "year": 2013
    },
    {
        "class": "Platinum",
        "href": "http://www.nvidia.com/",
        "src": "/attachments/supporters/tmp/nvidia.jpg",
        "year": 2013
    },
    {
        "class": "Gold",
        "href": "http://www.microsoft.com/",
        "src": "/attachments/supporters/tmp/ms-logo.jpg",
        "year": 2013
    },
    {
        "class": "Silver",
        "href": "http://www.agilent.com/",
        "src": "/attachments/supporters/tmp/agilent_logo.png",
        "year": 2013
    },
    {
        "class": "Silver",
        "href": "http://www.ibm.com/",
        "src": "/attachments/supporters/tmp/ibm-logo.png",
        "year": 2013
    },
    {
        "class": "Silver",
        "href": "http://www.kaust.edu.sa/",
        "src": "/attachments/supporters/tmp/Untitled.png",
        "year": 2013
    },
    {
        "class": "Bronze",
        "href": "http://www.autodesk.com/",
        "src": "/attachments/supporters/tmp/autodesk-logo-rgb-color-logo-black-text-medium.png",
        "year": 2013
    },
    {
        "class": "Bronze",
        "href": "http://www.mathmed.org/",
        "src": "/attachments/supporters/tmp/battelle.png",
        "year": 2013
    },
    {
        "class": "Bronze",
        "href": "http://www.kitware.com/",
        "src": "/attachments/supporters/tmp/kitware_0.gif",
        "year": 2013
    },
    {
        "class": "Bronze",
        "href": "http://www.pnnl.gov/",
        "src": "/attachments/supporters/tmp/PNNL_Color_Logo_Horizontal1_os.png",
        "year": 2013
    },
    {
        "class": "Bronze",
        "href": "http://www.sci.utah.edu/",
        "src": "/attachments/supporters/tmp/sci-logo-one-color.png",
        "year": 2013
    },
    {
        "class": "Bronze",
        "href": "http://www.tableausoftware.com/",
        "src": "/attachments/supporters/tmp/tableau_cmyk.png",
        "year": 2013
    },
    {
        "class": "Bronze",
        "href": "http://www.vis-sense.eu/",
        "src": "/attachments/supporters/tmp/VIS-SENSE_Logo_Web.png",
        "year": 2013
    },
    {
        "class": "Bronze",
        "href": "http://www.zspace.com/",
        "src": "/attachments/supporters/tmp/zspace.gif",
        "year": 2013
    },
    {
        "class": "Start Up/Small Company Sponsor",
        "href": "http://forio.com/",
        "src": "/attachments/supporters/tmp/VECTOR_LOGO.png",
        "year": 2013
    },
    {
        "class": "Start Up/Small Company Sponsor",
        "href": "http://www.zoomdata.com/",
        "src": "/attachments/supporters/tmp/ZD_logo_big_cropped.png",
        "year": 2013
    },
    {
        "class": "Publisher",
        "href": "http://www.crcpress.com/",
        "src": "/attachments/supporters/tmp/CRCblue.jpg",
        "year": 2013
    },
    {
        "class": "Academic",
        "href": "http://www.cc.gatech.edu/",
        "src": "/attachments/supporters/tmp/GT-logo.png",
        "year": 2013
    },
    {
        "class": "Academic",
        "href": "http://www.mica.edu/",
        "src": "/attachments/supporters/tmp/mica-logo.png",
        "year": 2013
    },
    {
        "class": "Academic",
        "href": "http://www.purdue.edu/discoverypark/vaccine/index.php",
        "src": "/attachments/supporters/tmp/vaccine_logo.png",
        "year": 2013
    },
    {
        "class": "Non-Profit",
        "href": "http://www.anl.gov/",
        "src": "/attachments/supporters/tmp/argonne.jpg",
        "year": 2013
    },
    {
        "class": "Diamond",
        "href": "http://www.inria.fr/",
        "src": "/attachments/supporters/tmp/in.png",
        "year": 2014
    },
    {
        "class": "Diamond",
        "href": "http://www.tableausoftware.com/",
        "src": "/attachments/supporters/tmp/tableau_cmyk_lg.jpg",
        "year": 2014
    },
    {
        "class": "Platinum",
        "href": "http://www.cea.fr/",
        "src": "/attachments/supporters/tmp/CEA_GB_logotype.jpg",
        "year": 2014
    },
    {
        "class": "Platinum",
        "href": "http://www.jcdecaux.com/en/",
        "src": "/attachments/supporters/tmp/JCDecaux.png",
        "year": 2014
    },
    {
        "class": "Gold",
        "href": "http://www.digiteo.fr/-en-",
        "src": "/attachments/supporters/tmp/digiteo%202-logo%20gb%20baseline.png",
        "year": 2014
    },
    {
        "class": "Gold",
        "href": "https://www.igd.fraunhofer.de/",
        "src": "/attachments/supporters/tmp/fr.png",
        "year": 2014
    },
    {
        "class": "Gold",
        "href": "http://www.microsoft.com/",
        "src": "/attachments/supporters/tmp/ms-logo_0.jpg",
        "year": 2014
    },
    {
        "class": "Silver",
        "href": "http://www.autodesk.com/",
        "src": "/attachments/supporters/tmp/autodesk2.png",
        "year": 2014
    },
    {
        "class": "Silver",
        "href": "http://www.campus-paris-saclay.fr/en/Idex-Paris-Saclay/Les-Lidex/Paris-Saclay-Center-for-Data-Science",
        "src": "/attachments/supporters/tmp/CDS.png",
        "year": 2014
    },
    {
        "class": "Silver",
        "href": "http://here.com/",
        "src": "/attachments/supporters/tmp/here.JPG",
        "year": 2014
    },
    {
        "class": "Bronze",
        "href": "http://www.adobe.com/",
        "src": "/attachments/supporters/tmp/adobe_logo_standard.png",
        "year": 2014
    },
    {
        "class": "Bronze",
        "href": "http://edf/com",
        "src": "/attachments/supporters/tmp/edf.png",
        "year": 2014
    },
    {
        "class": "Bronze",
        "href": "http://google.com/",
        "src": "/attachments/supporters/tmp/google.png",
        "year": 2014
    },
    {
        "class": "Bronze",
        "href": "http://www.ibm.com/",
        "src": "/attachments/supporters/tmp/IBM_logo.png",
        "year": 2014
    },
    {
        "class": "Bronze",
        "href": "http://www.iscpif.fr/",
        "src": "/attachments/supporters/tmp/logo_m_orange-DIM.png",
        "year": 2014
    },
    {
        "class": "Bronze",
        "href": "http://www.irt-systemx.fr/",
        "src": "/attachments/supporters/tmp/sx.png",
        "year": 2014
    },
    {
        "class": "Bronze",
        "href": "http://www.kitware.com/",
        "src": "/attachments/supporters/tmp/PrintLogo_NoGradient.png",
        "year": 2014
    },
    {
        "class": "Bronze",
        "href": "http://www.nvidia.com/",
        "src": "/attachments/supporters/tmp/NVLogo_2D.JPG",
        "year": 2014
    },
    {
        "class": "Bronze",
        "href": "http://www.scvis.fr/",
        "src": "/attachments/supporters/tmp/LOGO-SCV2.png",
        "year": 2014
    },
    {
        "class": "Bronze",
        "href": "http://www.techviz.net/",
        "src": "/attachments/supporters/tmp/techviz.jpg",
        "year": 2014
    },
    {
        "class": "Bronze",
        "href": "http://www.telecom-paristech.fr/nc/formation-et-innovation-dans-le-numerique.html",
        "src": "/attachments/supporters/tmp/logo-TPT.png",
        "year": 2014
    },
    {
        "class": "Publisher",
        "href": "http://www.crcpress.com/",
        "src": "/attachments/supporters/tmp/CRC_RGB2.png",
        "year": 2014
    },
    {
        "class": "Publisher",
        "href": "http://www.morganclaypool.com/",
        "src": "/attachments/supporters/tmp/MCP_logo_02.jpg",
        "year": 2014
    },
    {
        "class": "Publisher",
        "href": "http://www.springer.com/",
        "src": "/attachments/supporters/tmp/Springer.jpg",
        "year": 2014
    },
    {
        "class": "Start Up/Small Company Sponsor",
        "href": "http://www.aldecis.com/",
        "src": "/attachments/supporters/tmp/aldecis-logo.jpg",
        "year": 2014
    },
    {
        "class": "Start Up/Small Company Sponsor",
        "href": "https://www.dkrz.de/",
        "src": "/attachments/supporters/tmp/dkrz_logo2.png",
        "year": 2014
    },
    {
        "class": "Start Up/Small Company Sponsor",
        "href": "http://www.nsa.gov/",
        "src": "/attachments/supporters/tmp/NSA2.jpg",
        "year": 2014
    },
    {
        "class": "Academic",
        "href": "http://www.anl.gov/",
        "src": "/attachments/supporters/tmp/ANL_4C_P_H2.png",
        "year": 2014
    },
    {
        "class": "Academic",
        "href": "http://www.ocadu.ca/",
        "src": "/attachments/supporters/tmp/OCAD.png",
        "year": 2014
    },
    {
        "class": "Academic",
        "href": "http://www.sci.utah.edu/",
        "src": "/attachments/supporters/tmp/SCI-logo-transparent-black-med.png",
        "year": 2014
    },
    {
        "class": "Academic",
        "href": "http://www.visualdecision.fr/",
        "src": "/attachments/supporters/tmp/logo-vd-petit.jpg",
        "year": 2014
    },
    {
        "class": "Academic",
        "href": "http://www.visus.uni-stuttgart.de/en/institute.html",
        "src": "/attachments/supporters/tmp/VISUS2.png",
        "year": 2014
    },
    {
        "class": "Academic",
        "href": "http://www.vrvis.at/",
        "src": "/attachments/supporters/tmp/VRVis-Logo.png",
        "year": 2014
    },
    {
        "class": "Diamond",
        "href": "http://www.nsf.gov/",
        "src": "/attachments/supporters/tmp/nsf_t.png",
        "year": 2015
    },
    {
        "class": "Diamond",
        "href": "http://www.tableausoftware.com/",
        "src": "/attachments/supporters/tmp/tab.png",
        "year": 2015
    },
    {
        "class": "Platinum",
        "href": "http://www.autodesk.com/",
        "src": "/attachments/supporters/tmp/autodesk_2015.png",
        "year": 2015
    },
    {
        "class": "Platinum",
        "href": "http://www.intel.com/",
        "src": "/attachments/supporters/tmp/intel_2015.png",
        "year": 2015
    },
    {
        "class": "Gold",
        "href": "http://www.bloomberg.com/ux",
        "src": "/attachments/supporters/tmp/bloomberg_t.png",
        "year": 2015
    },
    {
        "class": "Gold",
        "href": "http://www.nvidia.com/",
        "src": "/attachments/supporters/tmp/NVLogo_2D.PNG",
        "year": 2015
    },
    {
        "class": "Silver",
        "href": "http://www.adobe.com/",
        "src": "/attachments/supporters/tmp/adobe_2015.png",
        "year": 2015
    },
    {
        "class": "Silver",
        "href": "http://att.com/",
        "src": "/attachments/supporters/tmp/att1.png",
        "year": 2015
    },
    {
        "class": "Silver",
        "href": "http://www.disneyresearch.com/",
        "src": "/attachments/supporters/tmp/DR.png",
        "year": 2015
    },
    {
        "class": "Silver",
        "href": "http://www.google.com/",
        "src": "/attachments/supporters/tmp/google2015.png",
        "year": 2015
    },
    {
        "class": "Silver",
        "href": "http://iacs.seas.harvard.edu/",
        "src": "/attachments/supporters/tmp/iacs.png",
        "year": 2015
    },
    {
        "class": "Silver",
        "href": "http://ibm.com/",
        "src": "/attachments/supporters/tmp/IBM_t.png",
        "year": 2015
    },
    {
        "class": "Silver",
        "href": "http://www.kaust.edu.sa/",
        "src": "/attachments/supporters/tmp/kaust.png",
        "year": 2015
    },
    {
        "class": "Silver",
        "href": "http://www.kitware.com/",
        "src": "/attachments/supporters/tmp/kitware.png",
        "year": 2015
    },
    {
        "class": "Silver",
        "href": "http://research.microsoft.com/",
        "src": "/attachments/supporters/tmp/MSR_t.png",
        "year": 2015
    },
    {
        "class": "Silver",
        "href": "http://www.pnnl.gov/",
        "src": "/attachments/supporters/tmp/PNNL2.PNG",
        "year": 2015
    },
    {
        "class": "Silver",
        "href": "https://www.nlm.nih.gov/",
        "src": "/attachments/supporters/tmp/nlm.png",
        "year": 2015
    },
    {
        "class": "Silver",
        "href": "https://uncharted.software/",
        "src": "/attachments/supporters/tmp/Uncharted-rgb-Vertical_27Aug2015.png",
        "year": 2015
    },
    {
        "class": "Bronze",
        "href": "http://www.act.org/",
        "src": "/attachments/supporters/tmp/ACT-logo-Blue-cmyk.png",
        "year": 2015
    },
    {
        "class": "Bronze",
        "href": "https://www.continuum.io/",
        "src": "/attachments/supporters/tmp/Anaconda_Logo_0702.png",
        "year": 2015
    },
    {
        "class": "Bronze",
        "href": "http://www.mechdyne.com/",
        "src": "/attachments/supporters/tmp/mechdyne.png",
        "year": 2015
    },
    {
        "class": "Bronze",
        "href": "http://plot.ly/",
        "src": "/attachments/supporters/tmp/plotly_logo_for_web_outlined.png",
        "year": 2015
    },
    {
        "class": "Bronze",
        "href": "http://renci.org/",
        "src": "/attachments/supporters/tmp/renci_ss.png",
        "year": 2015
    },
    {
        "class": "Bronze",
        "href": "http://vize.io/",
        "src": "/attachments/supporters/tmp/New_Logo-VIZE-transparent-small.png",
        "year": 2015
    },
    {
        "class": "Academic",
        "href": "http://www.niu.edu/",
        "src": "/attachments/supporters/tmp/NIU_2015.png",
        "year": 2015
    },
    {
        "class": "Academic",
        "href": "https://www.orau.org/",
        "src": "/attachments/supporters/tmp/ORAU.png",
        "year": 2015
    },
    {
        "class": "Academic",
        "href": "http://www.sci.utah.edu/",
        "src": "/attachments/supporters/tmp/sci.png",
        "year": 2015
    },
    {
        "class": "Academic",
        "href": "http://www.uic.edu/",
        "src": "/attachments/supporters/tmp/uic1.png",
        "year": 2015
    },
    {
        "class": "Academic",
        "href": "https://www.visus.uni-stuttgart.de/en/institute.html",
        "src": "/attachments/supporters/tmp/VISUS.png",
        "year": 2015
    },
    {
        "class": "Academic",
        "href": "http://www.vrvis.at/",
        "src": "/attachments/supporters/tmp/vrvis_t.png",
        "year": 2015
    },
    {
        "class": "Publisher",
        "href": "https://www.crcpress.com/",
        "src": "/attachments/supporters/tmp/CRC.png",
        "year": 2015
    },
    {
        "class": "Publisher",
        "href": "http://www.morganclaypool.com/",
        "src": "/attachments/supporters/tmp/mcp.png",
        "year": 2015
    },
    {
        "class": "Publisher",
        "href": "http://www.springer.com/",
        "src": "/attachments/supporters/tmp/Springer_cmyk.png",
        "year": 2015
    },
    {
        "class": "Diamond",
        "href": "http://www.nsf.gov/",
        "src": "/attachments/supporters/tmp/nsf_t.png",
        "year": 2016
    },
    {
        "class": "Diamond",
        "href": "http://www.tableausoftware.com/",
        "src": "/attachments/supporters/tmp/tab.png",
        "year": 2016
    },
    {
        "class": "Platinum",
        "href": "http://www.nvidia.com/",
        "src": " /attachments/supporters/tmp/NVLogo_2D.PNG",
        "year": 2016
    },
    {
        "class": "Gold",
        "href": "http://www.ibm.com/",
        "src": "/attachments/supporters/tmp/IBM_t.png",
        "year": 2016
    },
    {
        "class": "Gold",
        "href": "http://www.intel.com/",
        "src": "/attachments/supporters/tmp/intel_2015.png",
        "year": 2016
    },
    {
        "class": "Gold",
        "href": "http://research.microsoft.com/",
        "src": "/attachments/supporters/tmp/MSR_t.png",
        "year": 2016
    },
    {
        "class": "Silver",
        "href": "http://www.adobe.com/",
        "src": "/attachments/supporters/tmp/adobe_2015.png",
        "year": 2016
    },
    {
        "class": "Silver",
        "href": "https://uncharted.software/",
        "src": "/attachments/supporters/tmp/Uncharted-rgb-Vertical_27Aug2015.png",
        "year": 2016
    },
    {
        "class": "Silver",
        "href": "http://iacs.seas.harvard.edu/",
        "src": "/attachments/supporters/tmp/iacs.png",
        "year": 2016
    },
    {
        "class": "Silver",
        "href": "https://bocoup.com/",
        "src": "/attachments/bocoup-datavis-logo-vertical-2016.png",
        "year": 2016
    },
    {
        "class": "Silver",
        "href": "http://www.kaust.edu.sa/",
        "src": "/attachments/supporters/tmp/kaust.png",
        "year": 2016
    },
    {
        "class": "Silver",
        "href": "http://www.disneyresearch.com/",
        "src": "/attachments/supporters/tmp/DR.png",
        "year": 2016
    },
    {
        "class": "Silver",
        "href": "https://www.nlm.nih.gov/",
        "src": "/attachments/supporters/2016/nlm.png",
        "year": 2016
    },
    {
        "class": "Silver",
        "href": "https://www.mica.edu/",
        "src": "/attachments/supporters/2016/mica.png",
        "year": 2016
    },
    {
	"class": "Bronze",
	"href": "http://elevenpeppers.com/",
	"src": "/attachments/supporters/eleven_peppers.png",
	"year": 2016
    },
    {
	"class": "Bronze",
	"href": "http://rstudio.com/",
	"src": "/attachments/supporters/tmp/RStudio.png",
	"year": 2016
    },
    {
	"class": "Bronze",
	"href": "http://www.capdigital.com/en/",
	"src": "/attachments/supporters/2016/cap_digital.png",
	"year": 2016
    },
    {
        "class": "Bronze",
        "href": "http://www.google.com/",
        "src": "/attachments/supporters/tmp/google2015.png",
        "year": 2016
    },
    {
        "class": "NonProfit/Small Company/Startup",
        "href": "http://www.act.org/",
        "src": "/attachments/supporters/tmp/ACT-logo-Blue-cmyk.png",
	"year": 2016
    },
    {
        "class": "NonProfit/Small Company/Startup",
        "href": "http://www.sentimetrix.com/",
        "src": "/attachments/supporters/2016/sentimetrix.png",
	"year": 2016
    },
    {
	"class": "Academic",
	"href": "http://www.sci.utah.edu/",
	"src": "/attachments/supporters/tmp/sci.png",
	"year": 2016
    },
    {
	"class": "Academic",
	"href": "http://www.vrvis.at/",
	"src": "/attachments/supporters/tmp/vrvis_t.png",
	"year": 2016
    },
    {
	"class": "Academic",
	"href": "http://citi.clemson.edu/viz/",
	"src": "/attachments/supporters/2016/clemson_ccit.png",
	"year": 2016
    },
    {
	"class": "Publisher",
	"href": "http://www.morganclaypool.com/",
	"src": "/attachments/supporters/tmp/mcp.png",
	"year": 2016
    },
    {
	"class": "Publisher",
	"href": "http://www.springer.com/",
	"src": "/attachments/supporters/tmp/Springer_cmyk.png",
	"year": 2016
    },
    {
        "class": "Publisher",
        "href": "https://www.crcpress.com/",
        "src": "/attachments/supporters/tmp/CRC.png",
        "year": 2016
    }
];

function loadSponsors() {
    // d3.json("/js/all_sponsors.json", function(json) {
        var json = sponsorsJson;
        var year = guessYear();
        var div = d3.select("#supporters");
        var currentClass;
        for (var i=0; i<json.length; ++i) {
            var o = json[i];
            if (o.year !== year)
                continue;
            if (o.class !== currentClass) {
                currentClass = o.class;
                div.append("br");
                div.append("div").classed("supporter-level", true).text(o.class);
            }
            div.append("center")
                .append("a").attr("href", o.href)
                .append("img").attr("src", o.src).attr("width", "120");
        }
    // });
}

// The neverending train of disgusting hacks continues.
if (guessYear() !== 2016) {
    d3.select(document.getElementById("important-dates").parentNode).style("display", "none");
}

</script><div class="sidebar-title" id="supporters">Supporters</br> <a href="http://ieeevis.org/year/2016/info/exhibition/supporters-and-exhibition">(Become one)</a></div><script>loadSponsors();</script></div>
</div>
        </div>
          </div> <!-- /container -->
  </div>
<!-- /layout -->
  <div id="footer"><div id="block-block-4" class="clear-block block block-block">
<!--
-->
  <div class="content"><div class="footer-image"><img src="http://ieeevis.org/sites/visweek.vgtc.org/files/footer/visweek12-footer.jpg"></div><div class="footer-message"> © 2012 IEEE. Sponsored by the IEEE Computer Society Visualization and Graphics Technical Committee.</div></div>
</div>
</div>
    <!--[if IE]></div><![endif]--></body>
</html>
